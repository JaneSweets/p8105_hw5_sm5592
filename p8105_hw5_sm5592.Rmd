---
title: "Homework 5"
author: "Shaolei Ma"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(latex2exp)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

First, import the data.

```{r}
homicide_df =
  read_csv("data/homicide-data.csv", show_col_types = FALSE)
```

The data contains `r nrow(homicide_df)` observations and `r ncol(homicide_df)` variables related to the victim's biographical information and the location and disposition of the case.

Then, do some data cleaning and analysis.

```{r}
homicide_df = 
  homicide_df |> 
  janitor::clean_names() |> 
  mutate(city_state = str_c(city, ", ", state))

homicide_city_df = 
  homicide_df |> 
  group_by(city_state) |> 
  summarise(
    homicide_num = n(), 
    unsolved_num = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest"))

homicide_city_df |> 
  head() |> 
  knitr::kable()
```

Then, the total number of homicides is `r sum(homicide_city_df$homicide_num)`, and the total number of unsolved homicides is `r sum(homicide_city_df$unsolved_num)`.

For the city of Baltimore, MD:
```{r}
baltimore_df =
  homicide_city_df |> 
  filter(city_state == "Baltimore, MD")

baltimore_prop = prop.test(pull(baltimore_df, unsolved_num), pull(baltimore_df, homicide_num))

baltimore_prop_df = 
  baltimore_prop |> 
  broom::tidy(baltimore_prop)
```

The estimated proportion of unsolved homicides is `r pull(baltimore_prop_df, estimate) |> round(3)`, and the confidence interval is [`r pull(baltimore_prop_df, conf.low) |> round(3)`, `r pull(baltimore_prop_df, conf.high) |> round(3)`].

```{r}
prop_tidy = function(x, n) {
  
  result_df =
    prop.test(x, n) |> 
    broom::tidy()
  
  tibble(
    estimate = pull(result_df, estimate),
    conf.low = pull(result_df, conf.low),
    conf.high = pull(result_df, conf.high)
  )
  
}

unsolved_prop_df = 
  homicide_city_df |> 
  mutate(prop_estimate = map2(unsolved_num, homicide_num, prop_tidy)) |> 
  unnest(prop_estimate)

unsolved_prop_df |> 
  head() |> 
  knitr::kable(digits = 3)
```

The `unsolved_prop_df` dataframe collects the proportion estimate and CI for each city.

```{r}
unsolved_prop_df |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(y = city_state, x = estimate)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high))
```

As illustrated in the plot, Tulsa has the lowest estimate and the widest confidence interval, while Chicago has the highest estimate and the most narrow confidence interval.

# Problem 2
```{r}
import_data = function(url_path = "data/problem2_data/", name) {
  
  read_csv(str_c(url_path, name), show_col_types = F) # quiet the message
  
}

participants_df =
  tibble(names = list.files("data/problem2_data/")) |> # start with file names
  mutate(dat = map(names, import_data, url_path = "data/problem2_data/")) |> # read in data
  unnest(dat) |> 
  mutate(names = str_remove(names, ".csv")) |> #delete suffix
  separate(names, into = c("arm", "id"), sep = "_") |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observation",
    names_prefix = "week_",
    names_transform = list(week = as.numeric)
  )

participants_df |> 
  ggplot(aes(x = week, y = observation, group = id, color = id)) +
  geom_line() +
  facet_grid(. ~ arm) +
  labs(title = "Observations in Different Arms Over Weeks")
```

From the spaghetti plot, the observation values of the experimental arm increase over time, while those of the control arm fluctuates within a certain range.

# Problem 3

For $\mu=0$:
```{r}
t_test_tidy =
  function(n, mu, sigma) {
    
      rnorm(n, mu, sigma) |> 
      t.test() |> 
      broom::tidy() |> 
      select(estimate, p.value)
    
  }

sim_results_df =
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) |> 
  mutate(test_result = map(mu, t_test_tidy, n = 30, sigma = 5)) |> 
  unnest(test_result)

sim_results_df |>
  head() |> 
  knitr::kable(digits = 3)
```

For $\mu={1,2,3,4,5,6}$:
```{r}
sim2_results_df = 
  expand_grid(
    mu = c(1:6),
    iter = 1:5000
  ) |> 
  mutate(test_result = map(mu, t_test_tidy, n = 30, sigma = 5)) |> 
  unnest(test_result)

sim2_results_df |> 
  group_by(mu) |> 
  summarise(power = sum(p.value < 0.05) / n()) |> 
  ggplot(aes(x = mu, y = power)) + 
  geom_point() +
  geom_line() +
  labs(title = TeX("Power for Different $\\mu$"), x = TeX("$\\mu$"))
```

The power and effect size are positively correlated. With the increase of the effect size from one to six, the power increases from near 0.2 to near 1.

```{r}
sim2_results_df |> 
  group_by(mu) |> 
  summarise(mean_estimate = mean(estimate)) |> 
  ggplot(aes(x = mu, y = mean_estimate)) +
  geom_line(aes(color = "all", lty = "all")) +
  geom_line(data = 
              sim2_results_df |>
              filter(p.value < 0.05) |> 
              group_by(mu) |> 
              summarise(mean_estimate = mean(estimate)),
            aes(color = "rejected", lty = "rejected")) +
  scale_color_manual(name = "", values = c("all" = "grey", "rejected" = "red")) +
  scale_linetype_manual(name = "", values = c("all" = 2, "rejected" = 1)) +
  labs(title = TeX("Average Estimate for Different $\\mu$"), x = TeX("$\\mu$"), y = TeX("Average Estimate of $\\hat{\\mu}$"))
```

When $\mu$ is small, the sample average of $\hat\mu$ across tests for which the null is rejected is larger than the true value, while in the case when $\mu$ is large ($=4,5,6$), the sample average of $\hat\mu$ for which the null is rejected is approximately equal to the true value.

The average estimate follows the distribution: $\bar X\sim N(\mu, \frac{\sigma^2}{n}).$ If to reject $H_0: \mu=0$, the estimate has to be larger than $z_{1-\alpha}\frac{\sigma}{\sqrt n}=1.96*\frac{5}{\sqrt{30}}\approx 1.7527.$ Therefore, as $\sigma$ and $n$ are equal, there will be higher possibility to reject the null as $\mu$ increases. When more $\hat\mu s$ are rejected, the gap between the average estimate and the real value is greater. Therefore, the gap decreases as $\mu$ increases.
